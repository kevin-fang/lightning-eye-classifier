{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_in = np.load('inputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = X_in.shape[1]\n",
    "n_classes = 2 # only two, as blue/not blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 24 # number of neurons in the first hidden layer; should be between input size and number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(np.float64, [None, input_size])\n",
    "Y_ = tf.placeholder(np.float64, [None, n_classes])\n",
    "pkeep = tf.placeholder(tf.float64)\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([input_size, K], stddev=0.1, dtype=np.float64))\n",
    "B1 = tf.Variable(tf.ones([K], dtype=np.float64)/10)\n",
    "W3 = tf.Variable(tf.truncated_normal([K, n_classes], stddev=0.1, dtype=np.float64))\n",
    "B3 = tf.Variable(tf.zeros([n_classes], dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y1 = tf.nn.tanh(tf.matmul(X, W1) + B1)\n",
    "Y2 = tf.nn.dropout(Y1, pkeep) # dropout a number of neurons to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ylogits = tf.matmul(Y2, W3) + B3\n",
    "Y3 = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(Y3, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(Y3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninit = tf.global_variables_initializer()\\nwith tf.Session() as sess:\\n    seed = 1\\n    tf.set_random_seed(seed)\\n    # TODO: shuffle order and retrain network on shuffled data\\n    sess.run(init)\\n    X_in = np.load(\\'inputs.npy\\')\\n    Y_in = np.load(\\'eyes.npy\\')\\n    \\n    X_train, X_test, y_train, y_test = train_test_split(X_in, Y_in, test_size=.2, random_state=seed)\\n    num_splits = 2\\n    n_epochs = 10\\n    \\n    X_batches = np.array_split(X_train, num_splits)\\n    Y_batches = np.array_split(y_train, num_splits)\\n    for epoch in range(n_epochs):\\n        for split in range(num_splits):\\n            sess.run(train_step, {X: X_batches[split], Y_: Y_batches[split], pkeep: .8})\\n        \\n            a, c = sess.run([accuracy, cross_entropy], feed_dict={X: X_test, Y_: y_test, pkeep: 1})\\n            print \"Epoch:\", epoch + 1, \"Batch Num:\", split + 1, \"Test accuracy:\", a, \"Loss:\", c\\n        \\n    print \"Final:\"\\n    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: X_test, Y_: y_test, pkeep: 1})\\n    print \"Accuracy on test data: \", a, \"loss:\", c\\n    \\n    \\n    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: X_in, Y_: Y_in, pkeep: 1})\\n    print \"Accuracy on all data (not an accurate reflection):\", a, \"loss:\", c\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    seed = 1\n",
    "    tf.set_random_seed(seed)\n",
    "    # TODO: shuffle order and retrain network on shuffled data\n",
    "    sess.run(init)\n",
    "    X_in = np.load('inputs.npy')\n",
    "    Y_in = np.load('eyes.npy')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_in, Y_in, test_size=.2, random_state=seed)\n",
    "    num_splits = 2\n",
    "    n_epochs = 10\n",
    "    \n",
    "    X_batches = np.array_split(X_train, num_splits)\n",
    "    Y_batches = np.array_split(y_train, num_splits)\n",
    "    for epoch in range(n_epochs):\n",
    "        for split in range(num_splits):\n",
    "            sess.run(train_step, {X: X_batches[split], Y_: Y_batches[split], pkeep: .8})\n",
    "        \n",
    "            a, c = sess.run([accuracy, cross_entropy], feed_dict={X: X_test, Y_: y_test, pkeep: 1})\n",
    "            print \"Epoch:\", epoch + 1, \"Batch Num:\", split + 1, \"Test accuracy:\", a, \"Loss:\", c\n",
    "        \n",
    "    print \"Final:\"\n",
    "    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: X_test, Y_: y_test, pkeep: 1})\n",
    "    print \"Accuracy on test data: \", a, \"loss:\", c\n",
    "    \n",
    "    \n",
    "    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: X_in, Y_: Y_in, pkeep: 1})\n",
    "    print \"Accuracy on all data (not an accurate reflection):\", a, \"loss:\", c\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Train accuracy: 0.585714285714 Loss: 70.6598722206\n",
      "Epoch: 1 Test accuracy: 0.625 Loss: 60.9888938734\n",
      "\n",
      "Epoch: 2 Train accuracy: 0.585714285714 Loss: 69.4297021237\n",
      "Epoch: 2 Test accuracy: 0.625 Loss: 59.9706104744\n",
      "\n",
      "Epoch: 3 Train accuracy: 0.628571428571 Loss: 68.6131143657\n",
      "Epoch: 3 Test accuracy: 0.625 Loss: 59.0398625247\n",
      "\n",
      "Epoch: 4 Train accuracy: 0.628571428571 Loss: 68.0947843834\n",
      "Epoch: 4 Test accuracy: 0.625 Loss: 58.584696636\n",
      "\n",
      "Epoch: 5 Train accuracy: 0.628571428571 Loss: 67.6121023849\n",
      "Epoch: 5 Test accuracy: 0.625 Loss: 58.3042085637\n",
      "\n",
      "Epoch: 6 Train accuracy: 0.657142857143 Loss: 66.7367545441\n",
      "Epoch: 6 Test accuracy: 0.625 Loss: 57.8377604483\n",
      "\n",
      "Epoch: 7 Train accuracy: 0.657142857143 Loss: 66.4450591645\n",
      "Epoch: 7 Test accuracy: 0.625 Loss: 57.5926715379\n",
      "\n",
      "Epoch: 8 Train accuracy: 0.657142857143 Loss: 66.4327646739\n",
      "Epoch: 8 Test accuracy: 0.625 Loss: 57.5665961082\n",
      "\n",
      "Epoch: 9 Train accuracy: 0.657142857143 Loss: 66.3951087321\n",
      "Epoch: 9 Test accuracy: 0.625 Loss: 57.5709824982\n",
      "\n",
      "Epoch: 10 Train accuracy: 0.657142857143 Loss: 66.2882462202\n",
      "Epoch: 10 Test accuracy: 0.625 Loss: 57.5839434354\n",
      "\n",
      "Epoch: 11 Train accuracy: 0.685714285714 Loss: 66.2192509303\n",
      "Epoch: 11 Test accuracy: 0.625 Loss: 57.6002685783\n",
      "\n",
      "Epoch: 12 Train accuracy: 0.685714285714 Loss: 66.0389933077\n",
      "Epoch: 12 Test accuracy: 0.625 Loss: 57.6196467101\n",
      "\n",
      "Epoch: 13 Train accuracy: 0.685714285714 Loss: 65.8429497009\n",
      "Epoch: 13 Test accuracy: 0.625 Loss: 57.6394581391\n",
      "\n",
      "Epoch: 14 Train accuracy: 0.685714285714 Loss: 65.829885101\n",
      "Epoch: 14 Test accuracy: 0.625 Loss: 57.654744438\n",
      "\n",
      "Epoch: 15 Train accuracy: 0.685714285714 Loss: 65.7206071262\n",
      "Epoch: 15 Test accuracy: 0.625 Loss: 57.6658333659\n",
      "\n",
      "Epoch: 16 Train accuracy: 0.685714285714 Loss: 65.6723116413\n",
      "Epoch: 16 Test accuracy: 0.625 Loss: 57.6724665787\n",
      "\n",
      "Epoch: 17 Train accuracy: 0.685714285714 Loss: 65.6385309386\n",
      "Epoch: 17 Test accuracy: 0.625 Loss: 57.6735066837\n",
      "\n",
      "Epoch: 18 Train accuracy: 0.685714285714 Loss: 65.6229915165\n",
      "Epoch: 18 Test accuracy: 0.625 Loss: 57.6730855905\n",
      "\n",
      "Epoch: 19 Train accuracy: 0.685714285714 Loss: 65.4376054332\n",
      "Epoch: 19 Test accuracy: 0.625 Loss: 57.6719494528\n",
      "\n",
      "Epoch: 20 Train accuracy: 0.685714285714 Loss: 65.364998983\n",
      "Epoch: 20 Test accuracy: 0.625 Loss: 57.6677830327\n",
      "\n",
      "Epoch: 21 Train accuracy: 0.685714285714 Loss: 65.2962961003\n",
      "Epoch: 21 Test accuracy: 0.625 Loss: 57.6626079084\n",
      "\n",
      "Epoch: 22 Train accuracy: 0.685714285714 Loss: 65.2819786086\n",
      "Epoch: 22 Test accuracy: 0.625 Loss: 57.6565536638\n",
      "\n",
      "Epoch: 23 Train accuracy: 0.685714285714 Loss: 65.2632744795\n",
      "Epoch: 23 Test accuracy: 0.625 Loss: 57.649744601\n",
      "\n",
      "Epoch: 24 Train accuracy: 0.685714285714 Loss: 65.1534356182\n",
      "Epoch: 24 Test accuracy: 0.625 Loss: 57.6450353533\n",
      "\n",
      "Epoch: 25 Train accuracy: 0.685714285714 Loss: 64.8468683641\n",
      "Epoch: 25 Test accuracy: 0.625 Loss: 57.6415804742\n",
      "\n",
      "Epoch: 26 Train accuracy: 0.685714285714 Loss: 64.765096586\n",
      "Epoch: 26 Test accuracy: 0.625 Loss: 57.6395325622\n",
      "\n",
      "Epoch: 27 Train accuracy: 0.685714285714 Loss: 64.7448580207\n",
      "Epoch: 27 Test accuracy: 0.75 Loss: 57.636609697\n",
      "\n",
      "Epoch: 28 Train accuracy: 0.685714285714 Loss: 64.7298822687\n",
      "Epoch: 28 Test accuracy: 0.75 Loss: 57.6345801569\n",
      "\n",
      "Epoch: 29 Train accuracy: 0.685714285714 Loss: 64.7152936299\n",
      "Epoch: 29 Test accuracy: 0.75 Loss: 57.6314102078\n",
      "\n",
      "Epoch: 30 Train accuracy: 0.685714285714 Loss: 64.7008619368\n",
      "Epoch: 30 Test accuracy: 0.75 Loss: 57.6276106992\n",
      "\n",
      "Epoch: 31 Train accuracy: 0.685714285714 Loss: 64.6862629684\n",
      "Epoch: 31 Test accuracy: 0.75 Loss: 57.6255998429\n",
      "\n",
      "Epoch: 32 Train accuracy: 0.685714285714 Loss: 64.6473970931\n",
      "Epoch: 32 Test accuracy: 0.75 Loss: 57.6252113791\n",
      "\n",
      "Epoch: 33 Train accuracy: 0.685714285714 Loss: 64.5457142267\n",
      "Epoch: 33 Test accuracy: 0.75 Loss: 57.625338451\n",
      "\n",
      "Epoch: 34 Train accuracy: 0.685714285714 Loss: 64.4331365048\n",
      "Epoch: 34 Test accuracy: 0.75 Loss: 57.6273658262\n",
      "\n",
      "Epoch: 35 Train accuracy: 0.7 Loss: 64.2915063905\n",
      "Epoch: 35 Test accuracy: 0.75 Loss: 57.6290616912\n",
      "\n",
      "Epoch: 36 Train accuracy: 0.7 Loss: 64.2538694412\n",
      "Epoch: 36 Test accuracy: 0.75 Loss: 57.6367501384\n",
      "\n",
      "Epoch: 37 Train accuracy: 0.7 Loss: 64.2395759447\n",
      "Epoch: 37 Test accuracy: 0.75 Loss: 57.6397450641\n",
      "\n",
      "Epoch: 38 Train accuracy: 0.7 Loss: 64.2244324649\n",
      "Epoch: 38 Test accuracy: 0.75 Loss: 57.6420259982\n",
      "\n",
      "Epoch: 39 Train accuracy: 0.7 Loss: 64.2075627957\n",
      "Epoch: 39 Test accuracy: 0.75 Loss: 57.6457931802\n",
      "\n",
      "Epoch: 40 Train accuracy: 0.7 Loss: 64.1359912908\n",
      "Epoch: 40 Test accuracy: 0.75 Loss: 57.6489860868\n"
     ]
    }
   ],
   "source": [
    "# possibly allow to train on the data already produced by the SVM? But the problem is that we don't get to see behind the model so it's not particularily usefl\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    seed = 1\n",
    "    # TODO: shuffle order and retrain network on shuffled data\n",
    "    sess.run(init)\n",
    "    X_in = np.load('inputs.npy')\n",
    "    Y_in = np.load('eyes.npy')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_in, Y_in, test_size=0.1)\n",
    "    \n",
    "    num_splits = 2\n",
    "    n_epochs = 100\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(train_step, {X: X_train, Y_: y_train, pkeep: .5})\n",
    "\n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict={X: X_train, Y_: y_train, pkeep: 1})\n",
    "        print \"\\nEpoch:\", epoch + 1, \"Train accuracy:\", a, \"Loss:\", c\n",
    "        \n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict={X: X_test, Y_: y_test, pkeep: 1})\n",
    "        print \"Epoch:\", epoch + 1, \"Test accuracy:\", a, \"Loss:\", c\n",
    "        \n",
    "    print \"Final:\"\n",
    "    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: X_test, Y_: y_test, pkeep: 1})\n",
    "    print \"Accuracy on all data: \", a, \"loss:\", c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
